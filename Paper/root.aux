\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{watkins1992q}
\citation{smith2006optimizer}
\citation{van2004rational}
\citation{van2010double}
\citation{van2013estimating}
\citation{d2016estimating}
\citation{lee2013bias}
\citation{NIPS2011_4251}
\citation{mohagheghi2007proportional}
\citation{Tewari2007}
\citation{schweighofer2003meta}
\citation{Kobayashi2009}
\citation{yoshida2013reinforcement}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}{section.1}}
\@writefile{toc}{\contentsline {section}{\numberline {II}Preliminaries}{2}{section.2}}
\newlabel{eq:qpol}{{1}{2}{Preliminaries}{equation.2.1}{}}
\newlabel{eq:qopt}{{2}{2}{Preliminaries}{equation.2.2}{}}
\newlabel{eq:qdec}{{3}{2}{Preliminaries}{equation.2.3}{}}
\newlabel{eq:rqtilde}{{4}{2}{Preliminaries}{equation.2.4}{}}
\newlabel{eq:qdecrqtilde}{{5}{2}{Preliminaries}{equation.2.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {III}The Proposed Method}{2}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-A}}Decomposition of the TD error}{2}{subsection.3.1}}
\newlabel{eq:rtilupdedate}{{6}{2}{Decomposition of the TD error}{equation.3.6}{}}
\newlabel{eq:qtildeupdate}{{7}{2}{Decomposition of the TD error}{equation.3.7}{}}
\newlabel{eq:cumulativeupdate}{{8}{2}{Decomposition of the TD error}{equation.3.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-B}}Analysis of the decomposed update}{2}{subsection.3.2}}
\citation{crites1996improving}
\citation{bao2008infinite}
\citation{franccois2015discount}
\citation{EvenDar2001}
\citation{watkins1992q}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-C}}Variance dependent learning rate}{3}{subsection.3.3}}
\newlabel{eq:alpha_eq}{{15}{3}{Variance dependent learning rate}{equation.3.15}{}}
\newlabel{eq:delta_eq}{{16}{3}{Variance dependent learning rate}{equation.3.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-D}}Discussion on convergence}{3}{subsection.3.4}}
\newlabel{eq:lr_cond}{{17}{3}{Discussion on convergence}{equation.3.17}{}}
\citation{watkins1992q}
\citation{van2010double}
\citation{d2016estimating}
\citation{NIPS2011_4251}
\citation{van2010double}
\citation{van2010double}
\citation{van2010double}
\citation{d2016estimating}
\newlabel{eq:alpha_smv}{{18}{4}{Discussion on convergence}{equation.3.18}{}}
\newlabel{eq:beta_delta_smv}{{21}{4}{Discussion on convergence}{equation.3.21}{}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Experimental Results}{4}{section.4}}
\newlabel{S:empirical}{{IV}{4}{Experimental Results}{section.4}{}}
\newlabel{F:hasselt_all_1}{{1(a)}{4}{Subfigure 1(a)}{subfigure.1.1}{}}
\newlabel{sub@F:hasselt_all_1}{{(a)}{4}{Subfigure 1(a)\relax }{subfigure.1.1}{}}
\newlabel{F:hasselt_all_08}{{1(b)}{4}{Subfigure 1(b)}{subfigure.1.2}{}}
\newlabel{sub@F:hasselt_all_08}{{(b)}{4}{Subfigure 1(b)\relax }{subfigure.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Mean reward per step (top) and maximum action-value estimate in the initial state (bottom) of all the other algorithms and of the best setting of RQ-Learning\xspace  for this experiment. Results are averaged over $10000$ experiments.}}{4}{figure.1}}
\newlabel{F:hasselt_all}{{1}{4}{Mean reward per step (top) and maximum action-value estimate in the initial state (bottom) of all the other algorithms and of the best setting of \alg for this experiment. Results are averaged over $10000$ experiments}{figure.1}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {$\alpha = \genfrac {}{}{}0{1}{n(s,a)}$}}}{4}{figure.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {$\alpha = \genfrac {}{}{}0{1}{n(s,a)^{0.8}}$}}}{4}{figure.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-A}}Noisy Grid World}{4}{subsection.4.1}}
\citation{Peters2010RelativeEP}
\newlabel{F:hasselt_qdec_1}{{2(a)}{5}{Subfigure 2(a)}{subfigure.2.1}{}}
\newlabel{sub@F:hasselt_qdec_1}{{(a)}{5}{Subfigure 2(a)\relax }{subfigure.2.1}{}}
\newlabel{F:hasselt_qdec_08}{{2(b)}{5}{Subfigure 2(b)}{subfigure.2.2}{}}
\newlabel{sub@F:hasselt_qdec_08}{{(b)}{5}{Subfigure 2(b)\relax }{subfigure.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Mean reward per step (top) and maximum action-value estimate in the initial state (bottom) of the best setting of RQ-Learning\xspace  for this experiment together with other less effective setting of RQ-Learning\xspace  . Results are averaged over $10000$ experiments.}}{5}{figure.2}}
\newlabel{F:hasselt_QDecs}{{2}{5}{Mean reward per step (top) and maximum action-value estimate in the initial state (bottom) of the best setting of \alg for this experiment together with other less effective setting of \alg . Results are averaged over $10000$ experiments}{figure.2}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {$\alpha = \genfrac {}{}{}0{1}{n(s,a)}$}}}{5}{figure.2}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {$\alpha = \genfrac {}{}{}0{1}{n(s,a)^{0.8}}$}}}{5}{figure.2}}
\newlabel{F:hasselt_qdectol}{{3(a)}{5}{Subfigure 3(a)}{subfigure.3.1}{}}
\newlabel{sub@F:hasselt_qdectol}{{(a)}{5}{Subfigure 3(a)\relax }{subfigure.3.1}{}}
\newlabel{F:hasselt_qdecwintol}{{3(b)}{5}{Subfigure 3(b)}{subfigure.3.2}{}}
\newlabel{sub@F:hasselt_qdecwintol}{{(b)}{5}{Subfigure 3(b)\relax }{subfigure.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Mean reward per step (top) and maximum action-value estimate in the initial state (bottom) of RQ-Learning\xspace  (Figure \ref  {F:hasselt_qdectol}) and windowed RQ-Learning\xspace  (Figure \ref  {F:hasselt_qdecwintol}) with different values of $\eta $ and $k = 0.8$. Results are averaged over $10000$ experiments.}}{5}{figure.3}}
\newlabel{F:hasselt_QDecTol}{{3}{5}{Mean reward per step (top) and maximum action-value estimate in the initial state (bottom) of \alg (Figure \ref {F:hasselt_qdectol}) and windowed \alg (Figure \ref {F:hasselt_qdecwintol}) with different values of $\eta $ and $k = 0.8$. Results are averaged over $10000$ experiments}{figure.3}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {RQ-Learning\xspace }}}{5}{figure.3}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Windowed RQ-Learning\xspace }}}{5}{figure.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-B}}Double Chain}{5}{subsection.4.2}}
\newlabel{F:double_chain_1_1}{{4(a)}{6}{Subfigure 4(a)}{subfigure.4.1}{}}
\newlabel{sub@F:double_chain_1_1}{{(a)}{6}{Subfigure 4(a)\relax }{subfigure.4.1}{}}
\newlabel{F:double_chain_1_51}{{4(b)}{6}{Subfigure 4(b)}{subfigure.4.2}{}}
\newlabel{sub@F:double_chain_1_51}{{(b)}{6}{Subfigure 4(b)\relax }{subfigure.4.2}{}}
\newlabel{F:double_chain_5_1}{{4(c)}{6}{Subfigure 4(c)}{subfigure.4.3}{}}
\newlabel{sub@F:double_chain_5_1}{{(c)}{6}{Subfigure 4(c)\relax }{subfigure.4.3}{}}
\newlabel{F:double_chain_5_51}{{4(d)}{6}{Subfigure 4(d)}{subfigure.4.4}{}}
\newlabel{sub@F:double_chain_5_51}{{(d)}{6}{Subfigure 4(d)\relax }{subfigure.4.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Maximum action-value estimate in state $1$ (\ref  {F:double_chain_1_1}, \ref  {F:double_chain_1_51}) and state $5$ (\ref  {F:double_chain_5_1}, \ref  {F:double_chain_5_51}). Results are averaged over $500$ experiments.}}{6}{figure.4}}
\newlabel{F:double_chain_q}{{4}{6}{Maximum action-value estimate in state $1$ (\ref {F:double_chain_1_1}, \ref {F:double_chain_1_51}) and state $5$ (\ref {F:double_chain_5_1}, \ref {F:double_chain_5_51}). Results are averaged over $500$ experiments}{figure.4}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {$\alpha = \genfrac {}{}{}0{1}{n(s,a)}$}}}{6}{figure.4}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {$\alpha = \genfrac {}{}{}0{1}{n(s,a)^{0.51}}$}}}{6}{figure.4}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {$\alpha = \genfrac {}{}{}0{1}{n(s,a)}$}}}{6}{figure.4}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {$\alpha = \genfrac {}{}{}0{1}{n(s,a)^{0.51}}$}}}{6}{figure.4}}
\newlabel{F:lrs_1_1}{{5(a)}{6}{Subfigure 5(a)}{subfigure.5.1}{}}
\newlabel{sub@F:lrs_1_1}{{(a)}{6}{Subfigure 5(a)\relax }{subfigure.5.1}{}}
\newlabel{F:lrs_1_51}{{5(b)}{6}{Subfigure 5(b)}{subfigure.5.2}{}}
\newlabel{sub@F:lrs_1_51}{{(b)}{6}{Subfigure 5(b)\relax }{subfigure.5.2}{}}
\newlabel{F:lrs_5_1}{{5(c)}{6}{Subfigure 5(c)}{subfigure.5.3}{}}
\newlabel{sub@F:lrs_5_1}{{(c)}{6}{Subfigure 5(c)\relax }{subfigure.5.3}{}}
\newlabel{F:lrs_5_51}{{5(d)}{6}{Subfigure 5(d)}{subfigure.5.4}{}}
\newlabel{sub@F:lrs_5_51}{{(d)}{6}{Subfigure 5(d)\relax }{subfigure.5.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Learning rate of the two actions in state $1$ (\ref  {F:lrs_1_1}, \ref  {F:lrs_1_51}) and state $5$ (\ref  {F:lrs_5_1}, \ref  {F:lrs_5_51}) for RQ-Learning with and without windowed variance estimation. Results are averaged over $500$ experiments.}}{6}{figure.5}}
\newlabel{F:double_chain_lr}{{5}{6}{Learning rate of the two actions in state $1$ (\ref {F:lrs_1_1}, \ref {F:lrs_1_51}) and state $5$ (\ref {F:lrs_5_1}, \ref {F:lrs_5_51}) for RQ-Learning with and without windowed variance estimation. Results are averaged over $500$ experiments}{figure.5}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {$\alpha = \genfrac {}{}{}0{1}{n(s,a)}$}}}{6}{figure.5}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {$\alpha = \genfrac {}{}{}0{1}{n(s,a)^{0.51}}$}}}{6}{figure.5}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {$\alpha = \genfrac {}{}{}0{1}{n(s,a)}$}}}{6}{figure.5}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {$\alpha = \genfrac {}{}{}0{1}{n(s,a)^{0.51}}$}}}{6}{figure.5}}
\newlabel{F:max_a_1}{{6(a)}{6}{Subfigure 6(a)}{subfigure.6.1}{}}
\newlabel{sub@F:max_a_1}{{(a)}{6}{Subfigure 6(a)\relax }{subfigure.6.1}{}}
\newlabel{F:max_a_51}{{6(b)}{6}{Subfigure 6(b)}{subfigure.6.2}{}}
\newlabel{sub@F:max_a_51}{{(b)}{6}{Subfigure 6(b)\relax }{subfigure.6.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Action with maximum value in state $1$ and state $9$ for Q-Learning and windowed RQ-Learning.}}{6}{figure.6}}
\newlabel{F:max_a}{{6}{6}{Action with maximum value in state $1$ and state $9$ for Q-Learning and windowed RQ-Learning}{figure.6}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {$\alpha = \genfrac {}{}{}0{1}{n(s,a)}$}}}{6}{figure.6}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {$\alpha = \genfrac {}{}{}0{1}{n(s,a)^{0.51}}$}}}{6}{figure.6}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Structure of the double-chain problem.}}{6}{figure.7}}
\newlabel{F:double-chain}{{7}{6}{Structure of the double-chain problem}{figure.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Structure of the grid world with holes problem.}}{7}{figure.8}}
\newlabel{F:grid_hole_map}{{8}{7}{Structure of the grid world with holes problem}{figure.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Mean reward per step (top) and maximum action-value estimate in the initial state (bottom) of all the other algorithms and of the best setting of RQ-Learning\xspace  for this experiment. Results are averaged over $10000$ experiments.}}{7}{figure.9}}
\newlabel{F:hole}{{9}{7}{Mean reward per step (top) and maximum action-value estimate in the initial state (bottom) of all the other algorithms and of the best setting of \alg for this experiment. Results are averaged over $10000$ experiments}{figure.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-C}}Grid World with Holes}{7}{subsection.4.3}}
\newlabel{F:sarsa_1}{{10(a)}{7}{Subfigure 10(a)}{subfigure.10.1}{}}
\newlabel{sub@F:sarsa_1}{{(a)}{7}{Subfigure 10(a)\relax }{subfigure.10.1}{}}
\newlabel{F:sarsa_08}{{10(b)}{7}{Subfigure 10(b)}{subfigure.10.2}{}}
\newlabel{sub@F:sarsa_08}{{(b)}{7}{Subfigure 10(b)\relax }{subfigure.10.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Mean reward per step (top) and maximum action-value estimate in the initial state (bottom) of SARSA and of the on-policy windowed version of RQ-Learning\xspace  for this experiment. Results are averaged over $1000$ experiments.}}{7}{figure.10}}
\newlabel{F:sarsa}{{10}{7}{Mean reward per step (top) and maximum action-value estimate in the initial state (bottom) of SARSA and of the on-policy windowed version of \alg for this experiment. Results are averaged over $1000$ experiments}{figure.10}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {$\alpha = \genfrac {}{}{}0{1}{n(s,a)}$}}}{7}{figure.10}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {$\alpha = \genfrac {}{}{}0{1}{n(s,a)^{0.8}}$}}}{7}{figure.10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-D}}On-policy learning}{7}{subsection.4.4}}
\bibstyle{IEEEtran}
\bibdata{biblio}
\bibcite{watkins1992q}{1}
\bibcite{smith2006optimizer}{2}
\bibcite{van2004rational}{3}
\bibcite{van2010double}{4}
\bibcite{van2013estimating}{5}
\bibcite{d2016estimating}{6}
\bibcite{lee2013bias}{7}
\bibcite{NIPS2011_4251}{8}
\bibcite{mohagheghi2007proportional}{9}
\bibcite{Tewari2007}{10}
\bibcite{schweighofer2003meta}{11}
\bibcite{Kobayashi2009}{12}
\bibcite{yoshida2013reinforcement}{13}
\bibcite{crites1996improving}{14}
\bibcite{bao2008infinite}{15}
\bibcite{franccois2015discount}{16}
\bibcite{EvenDar2001}{17}
\bibcite{Peters2010RelativeEP}{18}
\@writefile{toc}{\contentsline {section}{\numberline {V}Conclusion}{8}{section.5}}
\@writefile{toc}{\contentsline {section}{References}{8}{section*.1}}

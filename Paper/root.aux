\relax 
\citation{watkins1992q}
\citation{smith2006optimizer}
\citation{van2004rational}
\citation{van2010double}
\citation{van2013estimating}
\citation{d2016estimating}
\citation{NIPS2011_4251}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {II}Preliminaries}{1}}
\newlabel{eq:qpol}{{1}{2}}
\newlabel{eq:qopt}{{2}{2}}
\newlabel{eq:qdec}{{3}{2}}
\newlabel{eq:rqtilde}{{4}{2}}
\newlabel{eq:qdecrqtilde}{{5}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {III}The Proposed Method}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-A}}Decomposition of the TD Error}{2}}
\newlabel{eq:rtilupdedate}{{6}{2}}
\newlabel{eq:qtildeupdate}{{7}{2}}
\newlabel{eq:cumulativeupdate}{{8}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-B}}Analysis of the Decomposed Update}{2}}
\citation{watkins1992q}
\citation{van2010double}
\citation{d2016estimating}
\citation{NIPS2011_4251}
\citation{van2010double}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-C}}Variance dependent learning rate}{3}}
\newlabel{eq:delta_eq}{{16}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-D}}Convergence Properties}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Experimental Results}{3}}
\newlabel{S:empirical}{{IV}{3}}
\citation{van2010double}
\citation{van2010double}
\citation{d2016estimating}
\newlabel{F:hasselt_all_1}{{1(a)}{4}}
\newlabel{sub@F:hasselt_all_1}{{(a)}{4}}
\newlabel{F:hasselt_all_08}{{1(b)}{4}}
\newlabel{sub@F:hasselt_all_08}{{(b)}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Mean reward per step (top) and maximum action-value estimate in the initial state (bottom) of all the other algorithms and of the best setting of RQ-Learning\xspace  for this experiment. Results are averaged over 10000 experiments.}}{4}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {$\alpha = \genfrac {}{}{}0{1}{n(s,a)}$}}}{4}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {$\alpha = \genfrac {}{}{}0{1}{n(s,a)^{0.8}}$}}}{4}}
\newlabel{F:hasselt_all}{{1}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-A}}Noisy Grid World}{4}}
\newlabel{F:hasselt_qdec_1}{{2(a)}{4}}
\newlabel{sub@F:hasselt_qdec_1}{{(a)}{4}}
\newlabel{F:hasselt_qdec_08}{{2(b)}{4}}
\newlabel{sub@F:hasselt_qdec_08}{{(b)}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Mean reward per step (top) and maximum action-value estimate in the initial state (bottom) of the best setting of RQ-Learning\xspace  for this experiment together with other less effective setting of RQ-Learning\xspace  . Results are averaged over 10000 experiments.}}{4}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {$\alpha = \genfrac {}{}{}0{1}{n(s,a)}$}}}{4}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {$\alpha = \genfrac {}{}{}0{1}{n(s,a)^{0.8}}$}}}{4}}
\newlabel{F:hasselt_QDecs}{{2}{4}}
\citation{Peters2010RelativeEP}
\newlabel{F:double_chain_1_1}{{3(a)}{5}}
\newlabel{sub@F:double_chain_1_1}{{(a)}{5}}
\newlabel{F:double_chain_1_51}{{3(b)}{5}}
\newlabel{sub@F:double_chain_1_51}{{(b)}{5}}
\newlabel{F:double_chain_5_1}{{3(c)}{5}}
\newlabel{sub@F:double_chain_5_1}{{(c)}{5}}
\newlabel{F:double_chain_5_51}{{3(d)}{5}}
\newlabel{sub@F:double_chain_5_51}{{(d)}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Maximum action-value estimate in state 1 (3(a)\hbox {}, 3(b)\hbox {}) and state 5 (3(c)\hbox {}, 3(d)\hbox {}). Results are averaged over 500 experiments.}}{5}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {$\alpha = \genfrac {}{}{}0{1}{n(s,a)}$}}}{5}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {$\alpha = \genfrac {}{}{}0{1}{n(s,a)^{0.51}}$}}}{5}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {$\alpha = \genfrac {}{}{}0{1}{n(s,a)}$}}}{5}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {$\alpha = \genfrac {}{}{}0{1}{n(s,a)^{0.51}}$}}}{5}}
\newlabel{F:double_chain_q}{{3}{5}}
\newlabel{F:lrs_1_1}{{4(a)}{5}}
\newlabel{sub@F:lrs_1_1}{{(a)}{5}}
\newlabel{F:lrs_1_51}{{4(b)}{5}}
\newlabel{sub@F:lrs_1_51}{{(b)}{5}}
\newlabel{F:lrs_5_1}{{4(c)}{5}}
\newlabel{sub@F:lrs_5_1}{{(c)}{5}}
\newlabel{F:lrs_5_51}{{4(d)}{5}}
\newlabel{sub@F:lrs_5_51}{{(d)}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Learning rate of the two actions in state 1 (4(a)\hbox {}, 4(b)\hbox {}) and state 5 (4(c)\hbox {}, 4(d)\hbox {}) for RQ-Learning with and without windowed variance estimation. Results are averaged over 500 experiments.}}{5}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {$\alpha = \genfrac {}{}{}0{1}{n(s,a)}$}}}{5}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {$\alpha = \genfrac {}{}{}0{1}{n(s,a)^{0.51}}$}}}{5}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {$\alpha = \genfrac {}{}{}0{1}{n(s,a)}$}}}{5}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {$\alpha = \genfrac {}{}{}0{1}{n(s,a)^{0.51}}$}}}{5}}
\newlabel{F:double_chain_lr}{{4}{5}}
\newlabel{F:max_a_1}{{5(a)}{5}}
\newlabel{sub@F:max_a_1}{{(a)}{5}}
\newlabel{F:max_a_51}{{5(b)}{5}}
\newlabel{sub@F:max_a_51}{{(b)}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Action with maximum value in state $1$ and state $9$ for Q-Learning and windowed RQ-Learning.}}{5}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {$\alpha = \genfrac {}{}{}0{1}{n(s,a)}$}}}{5}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {$\alpha = \genfrac {}{}{}0{1}{n(s,a)^{0.51}}$}}}{5}}
\newlabel{F:max_a}{{5}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Structure of the double-chain problem.}}{5}}
\newlabel{F:double-chain}{{6}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-B}}Double Chain}{5}}
\bibstyle{IEEEtran}
\bibdata{biblio}
\bibcite{watkins1992q}{1}
\bibcite{smith2006optimizer}{2}
\bibcite{van2004rational}{3}
\bibcite{van2010double}{4}
\bibcite{van2013estimating}{5}
\bibcite{d2016estimating}{6}
\bibcite{NIPS2011_4251}{7}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Structure of the grid world with holes problem.}}{6}}
\newlabel{F:grid_hole_map}{{7}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-C}}Grid World with Holes}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Mean reward per step (top) and maximum action-value estimate in the initial state (bottom) of all the other algorithms and of the best setting of RQ-Learning\xspace  for this experiment. Results are averaged over 10000 experiments.}}{6}}
\newlabel{F:hole}{{8}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Conclusion}{6}}
\@writefile{toc}{\contentsline {section}{References}{6}}
\bibcite{Peters2010RelativeEP}{8}

\documentclass[conference]{IEEEtran}



\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{algorithmic}
%\usepackage{url}

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}

\title{Reducing Uncertainty Propagation in Markov Decision Processes}


% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
\author{\IEEEauthorblockN{Davide Tateo, Alessandro Nuara, Carlo D'Eramo}
\IEEEauthorblockA{Department of Electronics, Information and Bioengineering\\
Politecnico Di Milano, Milano, Italy\\
Email: \{davide.tateo, alessandro.nuara, carlo.deramo\}@polimi.it}
}

% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract
\begin{abstract}
In many real-world problems stochasticity is a critical issue for the learning process. The sources of stochasticity come from the transition model, the explorative component of the policy or, even worse, from noisy observation of the reward function. For a finite number of samples, traditional reinforcement learning methods provide biased estimates of the action-value function. The presence of the bias leads to a poor estimation of the action-value function that is propagated to other action-values by the application of the Bellman operator. We propose an approach that significantly mitigates this issue avoiding the propagation of bad estimates of the action-value function.
\end{abstract}

% no keywords


% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle

\section{Introduction}
It is well known that a key issue of reinforcement learning problems is the accuracy of the estimation of the action-value with a limited number of samples. While most algorithms guarantee the convergence of the estimates to the optimal action-value, in practice the presence of stochastic components lead to poor performance. In fact, the majority of real-world problems have significant sources of stochasticity: the environment could be have stochastic transition and this complicates the estimation of the effectiveness of an action; most of the times it is necessary to use stochastic policies to guarantee that all states are visited infinitely many times, that is required to guarantee the convergence of the algorithm; the policy could change during the learning process resulting in very different behaviors; the reward function is often corrupted by noisy observations and, in other cases, the reward function is stochastic itself. Moreover, it usually happens that some deterministic environments are partially observable and, thus, are perceived by the agent as stochastic decision processes (e.g. Blackjack).

Since Monte-Carlo estimates of action-values are affected by high variance of the returns, the most successful reinforcement learning algorithms are based on bootstrapping (e.g. Q-Learning \cite{watkins1992q}), that trades off the variance of the estimation with a consistent but biased estimator. However with a finite number of samples the bias of the estimation could be significantly relevant when propagating the action-values to the next state and, recursively, it propagates to all the other states. Recent works tried to deal with this issue, in particular focusing on the estimation of the maximum expected value. It is well known \cite{smith2006optimizer, van2004rational} that the maximum operator (used in the Q-Learning update) is positively biased, thus it overestimates the optimal action-value. In highly stochastic environments, this overestimation leads to unstable learning and poor convergence rates. To avoid the presence of the positive bias, the Double Q-Learning algorithm \cite{van2010double} has been proposed. This algorithm uses the double estimator \cite{van2013estimating} that provides a negatively biased estimation of the action-values (i.e. it underestimates the optimal action-value) and this improves the performance when stochasticity is an issue. Another recently proposed approach is the Weighted Q-Learning \cite{d2016estimating} that balances between underestimation and overestimation. 

However, the correlation between bias of the estimation and performance is still unclear; indeed, as also shown in the empirical section of this paper, Speedy Q-Learning algorithm \cite{NIPS2011_4251} has very good performance despite its extremely poor estimation of the action-value. This is due to the fact that most of the policies are not dependent on the accuracy of the action-values, but instead they rely on their ordering. Starting from this consideration, we try to address this problem from another point of view. Indeed, we do not focus on the estimation of the maximum expected value, but we care about avoiding the propagation of the action-value estimates. In fact, we propose this work starting from the consideration that it is not relevant whether the approximations of the point estimates of the maximum expected value are good or not, but how these information is propagated to the other states. Our idea is to propagate the information about the value of the best action only when there is sufficiently certainty about the estimation.

The interesting aspect of this approach is that it is possible to use any maximum expected value estimator and, moreover, it can be easily extended in an on-policy scenario. Since we believe that in our approach the choice of the estimator is not a relevant issue, we choose the maximum operator as it is the simplest one.

\section{Preliminaries}

\section{The proposed Method}

\subsection{Decomposition of the TD error}

Decompose Q function:

\begin{align}
Q(x,u) & =\mathbb{E}\left[R(x,u,x')+\gamma Q(x',\pi(x'))\right] \nonumber\\ 
 & =\mathbb{E}\left[R(x,u,x')\right]+\gamma\mathbb{E}\left[Q(x',\pi(x'))\right] \nonumber\\
 & =\tilde{R}(x,u)+\gamma\tilde{Q}(x,u)
\end{align}

Decomposed TD update:

\begin{align}
\tilde{R}(x,u) & \leftarrow\tilde{R}(x,u)+\alpha(R(x,u,x')-\tilde{R}(x,u))\\
\tilde{Q}(x,u) & \leftarrow\tilde{Q}(x,u)+\beta(Q(x',\pi(x'))-\tilde{Q}(x,u))
\end{align}

Update of the Q function:

\begin{align}
Q(x,u) & \leftarrow\tilde{R}(x,u)+\alpha(R(x,u,x')-\tilde{R}(x,u)) \nonumber\\
 & +\gamma\left(\tilde{Q}(x,u)+\beta(Q(x',\pi(x'))-\tilde{Q}(x,u))\right) \nonumber\\
 & =Q(x,u)+\alpha(R(x,u,x')-\tilde{R}(x,u)) \nonumber\\
 & +\gamma\beta(Q(x',\pi(x'))-\tilde{Q}(x,u))
\end{align}

\subsection{Analysis of the decomposed update}
If $\alpha=\beta$

\begin{align}
Q(x,u) & \leftarrow Q(x,u)+\alpha(R(x,u,x')+\gamma Q(x',\pi(x')) \nonumber\\
 & -Q(x,u))
\end{align}

That is the classical Q-Learning update

If $\beta=\delta\alpha$
\begin{align}
Q(x,u) & \leftarrow Q(x,u)+\alpha(R(x,u,x')+\gamma\delta Q(x',\pi(x')) \nonumber\\
 & -(\tilde{R}(x,u)+\gamma\delta\tilde{Q}(x,u))) \nonumber\\
 & =Q(x,u)+\alpha(R(x,u,x')+\gamma'Q(x',\pi(x')) \nonumber\\
 & -(\tilde{R}(x,u)+\gamma'\tilde{Q}(x,u))) \nonumber\\
 & =Q(x,u)+\alpha((R(x,u,x')+\gamma'Q(x',\pi(x'))) \nonumber\\
 & -Q'(x,u))
\end{align}
With $\gamma'=\gamma\delta$. Notiche that $Q'(x,u)$ is the current Q function with a different learning rate.

\subsection{Variance dependent learning rate}
\begin{align}
 \alpha & =\dfrac{\sigma^{2}}{\sigma^{2}+1}
\end{align}

\section{Experimental results}

\section{Conclusion}


% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
\bibliography{biblio}



% that's all folks
\end{document}



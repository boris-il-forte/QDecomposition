% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{watkins1992q}
C.~J. Watkins and P.~Dayan, ``Q-learning,'' \emph{Machine learning}, vol.~8,
  no. 3-4, pp. 279--292, 1992.

\bibitem{smith2006optimizer}
J.~E. Smith and R.~L. Winkler, ``The optimizer's curse: Skepticism and
  postdecision surprise in decision analysis,'' \emph{Management Science},
  vol.~52, no.~3, pp. 311--322, 2006.

\bibitem{van2004rational}
E.~Van~den Steen, ``Rational overoptimism (and other biases),'' \emph{American
  Economic Review}, pp. 1141--1151, 2004.

\bibitem{van2010double}
H.~v. Hasselt, ``Double q-learning,'' in \emph{Advances in Neural Information
  Processing Systems}, 2010, pp. 2613--2621.

\bibitem{van2013estimating}
------, ``Estimating the maximum expected value: an analysis of (nested)
  cross-validation and the maximum sample average,'' \emph{arXiv preprint
  arXiv:1302.7175}, 2013.

\bibitem{van2016deep}
H.~Van~Hasselt, A.~Guez, and D.~Silver, ``Deep reinforcement learning with
  double q-learning.'' in \emph{AAAI}, 2016, pp. 2094--2100.

\bibitem{mnih2015human}
V.~Mnih, K.~Kavukcuoglu, D.~Silver, A.~A. Rusu, J.~Veness, M.~G. Bellemare,
  A.~Graves, M.~Riedmiller, A.~K. Fidjeland, G.~Ostrovski \emph{et~al.},
  ``Human-level control through deep reinforcement learning,'' \emph{Nature},
  vol. 518, no. 7540, pp. 529--533, 2015.

\bibitem{d2016estimating}
C.~D'Eramo, M.~Restelli, and A.~Nuara, ``Estimating maximum expected value
  through gaussian approximation,'' in \emph{International Conference on
  Machine Learning}, 2016, pp. 1032--1040.

\bibitem{lee2013bias}
D.~Lee, B.~Defourny, and W.~B. Powell, ``Bias-corrected q-learning to control
  max-operator bias in q-learning,'' in \emph{Adaptive Dynamic Programming And
  Reinforcement Learning (ADPRL), 2013 IEEE Symposium on}.\hskip 1em plus 0.5em
  minus 0.4em\relax IEEE, 2013, pp. 93--99.

\bibitem{NIPS2011_4251}
\BIBentryALTinterwordspacing
M.~Ghavamzadeh, H.~J. Kappen, M.~G. Azar, and R.~Munos, ``Speedy q-learning,''
  in \emph{Advances in Neural Information Processing Systems 24},
  J.~Shawe-Taylor, R.~S. Zemel, P.~L. Bartlett, F.~Pereira, and K.~Q.
  Weinberger, Eds.\hskip 1em plus 0.5em minus 0.4em\relax Curran Associates,
  Inc., 2011, pp. 2411--2419. [Online]. Available:
  \url{http://papers.nips.cc/paper/4251-speedy-q-learning.pdf}
\BIBentrySTDinterwordspacing

\bibitem{mohagheghi2007proportional}
S.~Mohagheghi, Y.~del Valle, G.~K. Venayagamoorthy, and R.~G. Harley, ``A
  proportional-integrator type adaptive critic design-based neurocontroller for
  a static compensator in a multimachine power system,'' \emph{IEEE
  Transactions on Industrial Electronics}, vol.~54, no.~1, pp. 86--96, 2007.

\bibitem{Tewari2007}
A.~Tewari and P.~L. Bartlett, \emph{Bounded Parameter Markov Decision Processes
  with Average Reward Criterion}, 2007, pp. 263--277.

\bibitem{schweighofer2003meta}
N.~Schweighofer and K.~Doya, ``Meta-learning in reinforcement learning,''
  \emph{Neural Networks}, vol.~16, no.~1, pp. 5--9, 2003.

\bibitem{Kobayashi2009}
K.~Kobayashi, H.~Mizoue, T.~Kuremoto, and M.~Obayashi, \emph{A Meta-learning
  Method Based on Temporal Difference Error}.\hskip 1em plus 0.5em minus
  0.4em\relax Springer Berlin Heidelberg, 2009, pp. 530--537.

\bibitem{yoshida2013reinforcement}
N.~Yoshida, E.~Uchibe, and K.~Doya, ``Reinforcement learning with
  state-dependent discount factor,'' in \emph{Development and Learning and
  Epigenetic Robotics (ICDL), 2013 IEEE Third Joint International Conference
  on}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2013, pp. 1--6.

\bibitem{crites1996improving}
R.~H. Crites and A.~G. Barto, ``Improving elevator performance using
  reinforcement learning,'' in \emph{Advances in neural information processing
  systems}, 1996, pp. 1017--1023.

\bibitem{bao2008infinite}
B.-K. Bao, B.-Q. Yin, and H.-S. Xi, ``Infinite-horizon policy-gradient
  estimation with variable discount factor for markov decision process,'' in
  \emph{Innovative Computing Information and Control, 2008. ICICIC'08. 3rd
  International Conference on}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE,
  2008, pp. 584--584.

\bibitem{franccois2015discount}
V.~Fran{\c{c}}ois-Lavet, R.~Fonteneau, and D.~Ernst, ``How to discount deep
  reinforcement learning: Towards new dynamic strategies,'' \emph{arXiv
  preprint arXiv:1512.02011}, 2015.

\bibitem{EvenDar2001}
\BIBentryALTinterwordspacing
E.~Even-Dar and Y.~Mansour, \emph{Learning Rates for Q-Learning}.\hskip 1em
  plus 0.5em minus 0.4em\relax Springer Berlin Heidelberg, 2001, pp. 589--604.
  [Online]. Available: \url{http://dx.doi.org/10.1007/3-540-44581-1_39}
\BIBentrySTDinterwordspacing

\bibitem{Peters2010RelativeEP}
J.~Peters, K.~Mulling, and Y.~Altun, ``Relative entropy policy search,'' in
  \emph{AAAI}, 2010.

\end{thebibliography}
